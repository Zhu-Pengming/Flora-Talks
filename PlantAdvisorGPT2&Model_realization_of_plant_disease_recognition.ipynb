{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPnUIFH7vlDufvHsZ4DlaLE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zhu-Pengming/Flora-Talks/blob/main/PlantAdvisorGPT2%26Model_realization_of_plant_disease_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu2niWcJK7vw",
        "outputId": "ab9c5e71-d47e-4516-a902-216057dfbc81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch 1, Batch 0, Loss: 8.295524597167969\n",
            "Epoch 1, Batch 10, Loss: 0.5927873253822327\n",
            "Epoch 2, Batch 0, Loss: 0.36712852120399475\n",
            "Epoch 2, Batch 10, Loss: 0.3388567268848419\n",
            "Epoch 3, Batch 0, Loss: 0.18605928122997284\n",
            "Epoch 3, Batch 10, Loss: 0.3709484040737152\n",
            "Generated Text: Q: How to solve powdery mildew?\n",
            "A: The most effective way to reduce the mildest disease that may occur with your new plants.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Set PAD token as EOS token if it's not already set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Define the path to your Excel file\n",
        "excel_file_path = '/content/drive/My Drive/dataset.xlsx'\n",
        "\n",
        "# Load data from Excel\n",
        "df = pd.read_excel(excel_file_path)\n",
        "texts = [f\"Q: {q}\\nA: {a}\" for q, a in zip(df.iloc[:, 0].tolist(), df.iloc[:, 1].tolist())]\n",
        "\n",
        "# Define the TextDataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=512):\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = item['input_ids'].clone()  # Labels are the same as input_ids\n",
        "        return item\n",
        "\n",
        "# Prepare data loader\n",
        "dataset = TextDataset(texts, tokenizer)\n",
        "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Set the computation device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training function\n",
        "def train(model, loader, epochs=3):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "    for epoch in range(epochs):\n",
        "        for batch_idx, batch in enumerate(loader):\n",
        "            inputs = batch['input_ids'].to(device)\n",
        "            masks = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=inputs, attention_mask=masks, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Epoch {epoch + 1}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
        "\n",
        "# Start training\n",
        "train(model, loader)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained('/content/drive/My Drive/3320/finetuned_gpt2')\n",
        "tokenizer.save_pretrained('/content/drive/My Drive/3320/finetuned_gpt2')\n",
        "\n",
        "# Import necessary libraries\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('/content/drive/My Drive/3320/finetuned_gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('/content/drive/My Drive/3320/finetuned_gpt2')\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(prompt, max_length=100):\n",
        "    # Encode the prompt\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', add_special_tokens=True)\n",
        "    inputs = {key: val.to(model.device) for key, val in inputs.items()}\n",
        "\n",
        "    # Generate text with sampling enabled\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        no_repeat_ngram_size=2,  # To encourage diversity in generated text\n",
        "        top_p=0.95,  # Use nucleus sampling\n",
        "        top_k=50,  # Use top-k sampling\n",
        "        do_sample=True  # Enable sampling\n",
        "    )\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Example prompt about preventing a plant disease\n",
        "prompt = \"Q: How to solve powdery mildew ?\\nA:\"\n",
        "\n",
        "# Generate text\n",
        "generated_text = generate_text(prompt)\n",
        "print(\"Generated Text:\", generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "应用"
      ],
      "metadata": {
        "id": "m5i2y5ApEyat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 16 * 16)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "num_classes = 2\n",
        "model = CNN(num_classes=num_classes)\n",
        "model_path = '/content/drive/My Drive/model.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "image_path = '/content/drive/My Drive/sample.jpg'\n",
        "image = Image.open(image_path)\n",
        "image = transform(image).unsqueeze(0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(image)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "\n",
        "disease_labels = {0: 'healthy', 1: 'diseased'}\n",
        "\n",
        "label = disease_labels[predicted[0].item()]\n",
        "prompt = f\"The plant leaf appears {label}. Please provide advice.\"\n",
        "print(prompt)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noccnIdqE1y0",
        "outputId": "104505b4-835c-4e1d-9a87-392ccc8e840a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "The plant leaf appears diseased. Please provide advice.\n"
          ]
        }
      ]
    }
  ]
}