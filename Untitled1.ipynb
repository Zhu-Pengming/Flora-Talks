{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMu/2pIHZqytjcj9p2e/5E0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zhu-Pengming/Flora-Talks/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icxSRthvpkvr",
        "outputId": "d2da50da-eab8-4d80-fef7-aa008680c432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 656kit/s]                                                      \n",
            "Fetching encoder.json: 1.00kit [00:00, 1.40Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.51Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.00kit [00:00, 1.51Mit/s]                                 \n",
            "Fetching model.ckpt.index: 1.00kit [00:00, 1.17Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 1.00kit [00:00, 4.32Mit/s]                                                \n",
            "Fetching vocab.bpe: 1.00kit [00:00, 4.29Mit/s]                                                      \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "model = \"text\"  # 直接在这里指定模型的名称\n",
        "\n",
        "subdir = os.path.join('models', model)\n",
        "if not os.path.exists(subdir):\n",
        "    os.makedirs(subdir)\n",
        "subdir = subdir.replace('\\\\','/') # needed for Windows\n",
        "\n",
        "for filename in ['checkpoint','encoder.json','hparams.json','model.ckpt.data-00000-of-00001', 'model.ckpt.index', 'model.ckpt.meta', 'vocab.bpe']:\n",
        "\n",
        "    r = requests.get(\"https://storage.googleapis.com/gpt-2/\" + subdir + \"/\" + filename, stream=True)\n",
        "\n",
        "    with open(os.path.join(subdir, filename), 'wb') as f:\n",
        "        file_size = int(r.headers[\"content-length\"])\n",
        "        chunk_size = 1000\n",
        "        with tqdm(ncols=100, desc=\"Fetching \" + filename, total=file_size, unit_scale=True) as pbar:\n",
        "            # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n",
        "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "                f.write(chunk)\n",
        "                pbar.update(chunk_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import importlib\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Flora')\n",
        "\n",
        "from src import encoder\n",
        "from src import net\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Args:\n",
        "  model = '124M'\n",
        "  model_ckpt = None\n",
        "  json_hparams = None\n",
        "  json_encoder = None\n",
        "  vocab_bpe = None\n",
        "  eager = False\n",
        "  dataset_path = [\"/content/drive/MyDrive/Flora/cleaned_powder.xlsx\", \"/content/drive/MyDrive/Flora/cleaned_rust.xlsx\"]\n",
        "  json_encoder = \"/content/drive/MyDrive/Flora/encoder.json\"\n",
        "  vocab_bpe = \"/content/drive/MyDrive/Flora/vocab.txt\"\n",
        "  num_epoch = 4\n",
        "  base_lr = 0.001\n",
        "  decay_lr = 0.1\n",
        "  decay_epochs = '1000,10000'\n",
        "  steps_per_epoch = 100\n",
        "  batch_size = 1\n",
        "  length = 1024\n",
        "  data_loader = 'text'\n",
        "  output_name = None\n",
        "\n",
        "args = Args()\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    if not args.eager:\n",
        "        tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "    if not args.json_encoder:\n",
        "        print('json_encoder must be provided.')\n",
        "        print('quit program.')\n",
        "        exit()\n",
        "\n",
        "    if not args.vocab_bpe:\n",
        "        print('vocab.bpe must be provided.')\n",
        "        print('quit program.')\n",
        "        exit()\n",
        "\n",
        "    if not os.path.exists('output'):\n",
        "        os.makedirs('output')\n",
        "\n",
        "    enc = encoder.get_encoder(args.json_encoder, args.vocab_bpe)\n",
        "\n",
        "    ds = importlib.import_module(\n",
        "        \"src.load_\" + args.data_loader).create_dataset(\n",
        "        enc, args.length, args.dataset_path, args.batch_size, args.steps_per_epoch, args.num_epoch)\n",
        "\n",
        "    # for value in ds.take(10):\n",
        "    #     x = enc.decode(value[0][0].numpy())\n",
        "    #     print(x)\n",
        "    #     print(len(value[0][0]))\n",
        "    #     input(\"Press Enter to continue...\")\n",
        "\n",
        "    # exit()\n",
        "\n",
        "    model = net.create_model(args)\n",
        "\n",
        "    # restore weight\n",
        "    model = net.load_weights(model, args)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        loss=net.loss\n",
        "    )\n",
        "\n",
        "    # fine tune\n",
        "    model.fit(ds,\n",
        "              epochs=args.num_epoch,\n",
        "              steps_per_epoch=args.steps_per_epoch,\n",
        "              callbacks=[LearningRateScheduler(net.create_schedule(args))])\n",
        "\n",
        "    model.save(os.path.join('output', args.output_name), include_optimizer=False)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "TzmkGXyhpocS",
        "outputId": "6fca9555-5047-40b3-823a-2098f0816679"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'encoded_comments' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-19ffdcdec8cc>\u001b[0m in \u001b[0;36m<cell line: 98>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-19ffdcdec8cc>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     ds = importlib.import_module(\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;34m\"src.load_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         enc, args.length, args.dataset_path, args.batch_size, args.steps_per_epoch, args.num_epoch)\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Flora/src/load_text.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(enc, length, dataset_paths, batch_size, steps_per_epoch, num_epoch)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mencoded_comments1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcomment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0;31m# Create a tf.data.Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_comments1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'encoded_comments' is not defined"
          ]
        }
      ]
    }
  ]
}