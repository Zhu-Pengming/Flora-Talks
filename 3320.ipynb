{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM512yM/cfBmJclHSMUod66",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zhu-Pengming/Flora-Talks/blob/main/3320.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu2niWcJK7vw",
        "outputId": "d15724f0-678f-4754-bea4-ec2ff95261dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch 1, Batch 0, Loss: 8.442889213562012\n",
            "Epoch 1, Batch 10, Loss: 0.39113685488700867\n",
            "Epoch 2, Batch 0, Loss: 0.4559739828109741\n",
            "Epoch 2, Batch 10, Loss: 0.28849339485168457\n",
            "Epoch 3, Batch 0, Loss: 0.3130795359611511\n",
            "Epoch 3, Batch 10, Loss: 0.18613019585609436\n",
            "Generated Text: Q: How to solve powdery mildew?\n",
            "A: Remove the yellow and white leaves from the stems of the plants.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Set PAD token as EOS token if it's not already set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Define the path to your Excel file\n",
        "excel_file_path = '/content/drive/My Drive/dataset.xlsx'\n",
        "\n",
        "# Load data from Excel\n",
        "df = pd.read_excel(excel_file_path)\n",
        "texts = [f\"Q: {q}\\nA: {a}\" for q, a in zip(df.iloc[:, 0].tolist(), df.iloc[:, 1].tolist())]\n",
        "\n",
        "# Define the TextDataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=512):\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = item['input_ids'].clone()  # Labels are the same as input_ids\n",
        "        return item\n",
        "\n",
        "# Prepare data loader\n",
        "dataset = TextDataset(texts, tokenizer)\n",
        "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Set the computation device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training function\n",
        "def train(model, loader, epochs=3):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "    for epoch in range(epochs):\n",
        "        for batch_idx, batch in enumerate(loader):\n",
        "            inputs = batch['input_ids'].to(device)\n",
        "            masks = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=inputs, attention_mask=masks, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Epoch {epoch + 1}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
        "\n",
        "# Start training\n",
        "train(model, loader)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained('/content/drive/My Drive/3320/finetuned_gpt2')\n",
        "tokenizer.save_pretrained('/content/drive/My Drive/3320/finetuned_gpt2')\n",
        "\n",
        "# Import necessary libraries\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('/content/drive/My Drive/3320/finetuned_gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('/content/drive/My Drive/3320/finetuned_gpt2')\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(prompt, max_length=100):\n",
        "    # Encode the prompt\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', add_special_tokens=True)\n",
        "    inputs = {key: val.to(model.device) for key, val in inputs.items()}\n",
        "\n",
        "    # Generate text with sampling enabled\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        no_repeat_ngram_size=2,  # To encourage diversity in generated text\n",
        "        top_p=0.95,  # Use nucleus sampling\n",
        "        top_k=50,  # Use top-k sampling\n",
        "        do_sample=True  # Enable sampling\n",
        "    )\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Example prompt about preventing a plant disease\n",
        "prompt = \"Q: How to solve powdery mildew ?\\nA:\"\n",
        "\n",
        "# Generate text\n",
        "generated_text = generate_text(prompt)\n",
        "print(\"Generated Text:\", generated_text)\n"
      ]
    }
  ]
}